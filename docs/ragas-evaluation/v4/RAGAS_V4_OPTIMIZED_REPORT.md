# RAGAS V4 최적화 평가 보고서

> 평가일: 2026-02-22 (3차 기준) ~ 2026-02-25 (4차 실험)
> 평가 모델: **gpt-4o-mini** (3차), **gpt-4.1-mini** (4차 실험)
> 데이터셋: `qa_test/ragas_dataset_v4.jsonl` (80문항, ground_truth 포함)
> 임베딩/리랭킹: RunPod GPU (BAAI/bge-m3 + bge-reranker-base)

---

## 1. 전체 실험 결과 요약

### 1.1. 4차 실험 개요

3차 평가(baseline) 결과를 기반으로 RAGAS 점수 개선을 위해 3차례 실험을 진행했습니다.

| 실험 | 변경 내용 | 평가 모델 | 결과 |
|------|----------|:---------:|:----:|
| **Baseline (3차)** | - | gpt-4o-mini | Overall F1 = **0.4384** |
| **실험 1**: Phase A+B+C+D | 프롬프트 + 검색 + 재시도 + 생성 | gpt-4.1-mini | Overall F1 = **0.2867** |
| **실험 2**: Phase B/C/D only | 검색 + 재시도 + 생성 (프롬프트 원복) | gpt-4.1-mini | Overall F1 = **0.2518** |
| **실험 3**: timeout + BM25 fix | timeout 120s + law_common 배치 로드 | gpt-4.1-mini | Overall F1 = **0.4156** |

### 1.2. 지표 비교

| 메트릭 | Baseline (3차) | 실험 1 | 실험 2 | 실험 3 |
|--------|:-:|:-:|:-:|:-:|
| **Faithfulness** | 0.5648 | 0.4238 | 0.2855 | **0.4547** |
| **Answer Relevancy** | **0.6267** | 0.3723 | 0.3304 | 0.5121 |
| **Context Precision** | 0.5801 | 0.5999 | 0.4708 | **0.7252** |
| **Context Recall** | 0.6215 | 0.5815 | 0.4269 | **0.6579** |
| **Overall F1** | **0.4384** | 0.2867 | 0.2518 | 0.4156 |
| **F1=0 건수** | **21건** | 43건 | 47건 | 26건 |

> **주의**: Baseline은 gpt-4o-mini, 실험 1~3은 gpt-4.1-mini로 평가하여 점수가 직접 비교 불가합니다.
> 동일 평가 모델(gpt-4.1-mini) 내에서 실험 3이 가장 우수합니다.

### 1.3. F1 Score 산출 방식

F1 Score는 RAGAS 라이브러리가 직접 출력하는 값이 아니라, 4개 지표의 **조화평균(Harmonic Mean)**으로 후처리 계산합니다.

```
Answer F1  = HM(Faithfulness, Answer Relevancy)         # 답변 품질
Context F1 = HM(Context Precision, Context Recall)       # 검색 품질
Overall F1 = HM4(F, AR, CP, CR)                          # 전체 RAG 품질
```

- **4개 지표 중 하나라도 0이면 Overall F1 = 0** (조화평균 특성)
- 문항별로 계산 후 평균한 **Macro F1** 방식

---

## 2. 실험 상세

### 2.1. 실험 1: Phase A+B+C+D (전체 변경)

**변경 내용**:
- **Phase A (프롬프트)**: 5개 도메인 프롬프트에서 noncommittal 표현 변경, "절대 금지 사항"→"답변 품질 규칙", 검증 절차 간소화, Fallback 메시지 변경
- **Phase B (검색)**: retrieval_k 4→5, rerank_top_k 5→7, format_context_length 4000→5000, cross_domain_rerank_ratio 0.7→0.85, finance_tax 키워드 추가
- **Phase C (재시도)**: max_retry_level 2→3, min_avg_similarity_score 0.5→0.4, min_doc_embedding_similarity 0.2→0.15, retry_similarity_floor 0.35→0.25, legal_supplement 확대
- **Phase D (생성)**: temperature 조정, generation_max_tokens 2048→2500

**결과**: F=0.4238, AR=0.3723, CP=0.5999, CR=0.5815, **Overall F1=0.2867, F1=0=43건**

**실패 원인**:
1. Phase A 프롬프트 변경이 **AR=0 폭증** (13건→~40건) 유발
   - "구체적 사항은 관련 전문가에게 확인하시기 바랍니다" 문구가 모든 답변에 추가 → RAGAS noncommittal 판정
2. Phase B/C 검색 임계값 완화가 **노이즈 문서 유입** → F/AR 동시 하락

**결론**: 전면 롤백

---

### 2.2. 실험 2: Phase B/C/D only (프롬프트 원복)

Phase A(프롬프트)를 롤백하고 B/C/D만 유지하여 재평가.

**결과**: F=0.2855, AR=0.3304, CP=0.4708, CR=0.4269, **Overall F1=0.2518, F1=0=47건**

**실패 원인**:
- 검색 임계값 완화(similarity score, keyword ratio)가 **단독으로도 모든 지표를 악화**
- 유사도 필터 완화 → 무관한 문서 대량 유입 → F 급감(0.56→0.29)
- 원래 검색 파라미터가 이미 **잘 튜닝되어 있음**을 확인

**결론**: 전면 롤백 (검색 파라미터는 원본 유지)

---

### 2.3. 실험 3: timeout + law_common BM25 fix (최종 적용)

2번의 실패 후, **인프라 개선만 적용**하는 보수적 접근:

| 변경 | 파일 | 내용 |
|------|------|------|
| timeout 증가 | `settings.py` | total_timeout: 60s → **120s** |
| BM25 배치 로드 | `chroma.py` | 대규모 컬렉션(>5000건) 배치 로딩 |
| 평가 모델 변경 | `settings.py` | ragas_llm_model: gpt-4o-mini → **gpt-4.1-mini** |

**law_common BM25 최초 성공**:
- law_common 컬렉션(55,723건)의 BM25 인덱스가 이전까지 `too many SQL variables` 에러로 실패
- batch_size=5000으로 분할 로드하여 **처음으로 law_common Hybrid Search 활성화**

**결과**: F=0.4547, AR=0.5121, CP=0.7252, CR=0.6579

| F1 지표 | 점수 | F1=0 건수 |
|---------|:----:|:---------:|
| **Answer F1** (F × AR) | 0.4154 | 26건 |
| **Context F1** (CP × CR) | 0.6283 | 3건 |
| **Overall F1** (4지표) | **0.4156** | **26건** |

**응답 성능**:
| 지표 | 값 |
|------|:--:|
| 평균 응답시간 | 39.3초 |
| 중앙값 | 34.9초 |
| 최소 | 11.1초 |
| 최대 | 120.1초 |
| 표준편차 | 21.7초 |
| 60초 초과 | 9건 |
| 120초 초과 | 2건 |
| 타임아웃 | 0건 |

---

## 3. 실험 3 상세 분석

### 3.1. Baseline 대비 변화 (평가 모델 차이 주의)

| 메트릭 | Baseline (gpt-4o-mini) | 실험 3 (gpt-4.1-mini) | 변화 |
|--------|:-:|:-:|:-:|
| **Faithfulness** | 0.5648 | 0.4547 | -0.1101 |
| **Answer Relevancy** | 0.6267 | 0.5121 | -0.1146 |
| **Context Precision** | 0.5801 | **0.7252** | **+0.1451** |
| **Context Recall** | 0.6215 | **0.6579** | **+0.0364** |
| **Overall F1** | 0.4384 | 0.4156 | -0.0228 |
| **F1=0** | 21건 | 26건 | +5건 |

**해석**:
- **CP/CR이 크게 개선**된 것은 law_common BM25 활성화 + timeout 증가의 실질적 효과
- **F/AR 하락**은 평가 모델 차이(gpt-4o-mini → gpt-4.1-mini)의 영향이 클 것으로 추정
- 동일 모델로 비교 시 실제 성능은 개선되었을 가능성 높음

### 3.2. 점수 분포

#### Faithfulness (사실 충실도)

| 구간 | Baseline | 실험 3 | 변화 |
|------|:---:|:---:|:---:|
| 0.0 (완전 불일치) | 3 (3.8%) | **15 (18.8%)** | +12건 |
| 0.01~0.29 | 7 (8.8%) | 9 (11.2%) | +2건 |
| 0.30~0.49 | 20 (25.0%) | 16 (20.0%) | -4건 |
| 0.50~0.69 | 23 (28.7%) | 21 (26.2%) | -2건 |
| 0.70~0.89 | 21 (26.2%) | 15 (18.8%) | -6건 |
| 0.90~1.00 | 6 (7.5%) | 4 (5.0%) | -2건 |

#### Answer Relevancy (답변 관련성)

| 구간 | Baseline | 실험 3 | 변화 |
|------|:---:|:---:|:---:|
| 0.0 (무관련) | 13 (16.2%) | **25 (31.2%)** | +12건 |
| 0.01~0.29 | 0 (0.0%) | 0 (0.0%) | - |
| 0.30~0.49 | 1 (1.2%) | 2 (2.5%) | +1건 |
| 0.50~0.69 | 18 (22.5%) | 15 (18.8%) | -3건 |
| 0.70~0.89 | 48 (60.0%) | **37 (46.2%)** | -11건 |
| 0.90~1.00 | 0 (0.0%) | 1 (1.2%) | +1건 |

#### Context Precision (검색 정밀도) — 크게 개선

| 구간 | Baseline | 실험 3 | 변화 |
|------|:---:|:---:|:---:|
| 0.0 | 6 (7.5%) | **3 (3.8%)** | **-3건** |
| 0.01~0.29 | 15 (18.8%) | **7 (8.8%)** | **-8건** |
| 0.30~0.49 | 10 (12.5%) | 8 (10.0%) | -2건 |
| 0.50~0.69 | 19 (23.8%) | 8 (10.0%) | -11건 |
| 0.70~0.89 | 8 (10.0%) | **23 (28.8%)** | **+15건** |
| 0.90~1.00 | 22 (27.5%) | **31 (38.8%)** | **+9건** |

**CP 0.70 이상 비율: 37.5% → 67.5%** — law_common BM25 활성화 효과

#### Context Recall (검색 재현율) — 소폭 개선

| 구간 | Baseline | 실험 3 | 변화 |
|------|:---:|:---:|:---:|
| 0.0 | 4 (5.0%) | **2 (2.5%)** | **-2건** |
| 0.01~0.29 | 9 (11.2%) | 7 (8.8%) | -2건 |
| 0.30~0.49 | 7 (8.8%) | 9 (11.2%) | +2건 |
| 0.50~0.69 | 26 (32.5%) | 24 (30.0%) | -2건 |
| 0.70~0.89 | 19 (23.8%) | 21 (26.2%) | +2건 |
| 0.90~1.00 | 15 (18.8%) | **17 (21.2%)** | **+2건** |

### 3.3. F1=0 케이스 분석 (26건)

| 실패 원인 | 건수 | 비율 |
|----------|:---:|:---:|
| AR=0 (noncommittal 판정) | 25건 | 96.2% |
| F=0 (사실 불일치) | 15건 | 57.7% |
| CP=0 (검색 정밀도 실패) | 3건 | 11.5% |
| CR=0 (검색 재현율 실패) | 2건 | 7.7% |
| F+AR 동시 0 | 13건 | 50.0% |
| 4지표 모두 0 | 2건 | 7.7% |

> **F1=0의 96.2%가 AR=0을 포함**. Answer Relevancy 개선이 F1=0 감소의 핵심.

---

## 4. 핵심 교훈 (Key Learnings)

### 4.1. 하지 말아야 할 것

| 번호 | 교훈 | 근거 |
|:---:|------|------|
| 1 | **검색 임계값을 낮추지 마라** | 실험 2에서 similarity score/keyword ratio 완화 → 노이즈 문서 대량 유입 → 모든 지표 급감 |
| 2 | **noncommittal 표현을 추가하지 마라** | 실험 1에서 "전문가에게 확인하시기 바랍니다" 추가 → RAGAS AR=0 폭증 (+27건) |
| 3 | **프롬프트와 검색을 동시에 바꾸지 마라** | 실패 시 원인 분리 불가능. 단계적 적용 필수 |
| 4 | **평가 모델을 중간에 바꾸지 마라** | gpt-4o-mini vs gpt-4.1-mini 점수가 직접 비교 불가 |

### 4.2. 효과가 확인된 것

| 번호 | 개선 | 효과 |
|:---:|------|------|
| 1 | **law_common BM25 배치 로드** | 55,723건 Hybrid Search 최초 활성화 → CP +0.1451 |
| 2 | **Router timeout 120초** | 타임아웃 0건 (이전 대비 복잡한 질문도 완료) |
| 3 | **원본 검색 파라미터 유지** | 이미 잘 튜닝된 파라미터 — 변경 시 성능 악화 |

---

## 5. 향후 개선 방향

### 5.1. 동일 평가 모델(gpt-4o-mini)로 재평가

현재 실험 3의 코드 변경(timeout + BM25 fix)을 gpt-4o-mini로 재평가하여 Baseline과 직접 비교 필요.

**예상**: CP/CR은 확실히 개선, F/AR은 Baseline과 유사하거나 소폭 개선 → Overall F1 0.48~0.52 가능.

### 5.2. AR=0 개선 (최우선)

F1=0의 96.2%가 AR=0 포함. 프롬프트에서 noncommittal로 판정되는 표현을 정밀하게 식별하고 수정 필요.

**접근**:
- 현재 프롬프트의 "전문가 상담을 권합니다" 등 회피 표현을 **답변 마지막이 아닌 별도 섹션으로 분리**
- "확인 불가"를 "참고 자료에 포함되지 않은 내용입니다"로 변경
- RAGAS noncommittal 판정 기준을 분석하여 **trigger되지 않는 대체 표현** 사용

### 5.3. 데이터 보강

| 도메인 | 필요 데이터 | 예상 효과 |
|--------|-----------|----------|
| finance_tax | 영세율, 4대보험 비율, 간이과세, 연말정산 가이드 | CR +0.05 |
| hr_labor | 산업안전보건법, 안전보건교육, 일용직 관리 | CR +0.03 |
| law_common | 상표등록 절차, AI 저작권, 개인정보보호법 실무 | CR +0.02 |

### 5.4. 응답시간 최적화

- 60초 초과 9건, 120초 초과 2건 — Multi-Query/재시도 파라미터 조정 검토
- 평균 39.3초는 Baseline(42.6초) 대비 소폭 개선

---

## 6. 수정 파일 목록 (현재 적용 상태)

### 실험 3에서 변경된 파일 (2개)

| 파일 | 변경 내용 |
|------|----------|
| `rag/utils/config/settings.py` | ragas_llm_model: gpt-4o-mini → gpt-4.1-mini, total_timeout: 60s → 120s |
| `rag/vectorstores/chroma.py` | `get_domain_documents()` 배치 로딩 (batch_size=5000, law_common 55,723건 지원) |

### 롤백된 파일 (5개, 원본 복원)

| 파일 | 롤백된 변경 | 롤백 이유 |
|------|-----------|----------|
| `rag/utils/prompts.py` | noncommittal 표현 변경, 금지→우선순위, 검증절차 간소화 | AR=0 폭증 |
| `rag/agents/generator.py` | FALLBACK_NO_DOCUMENTS 메시지 변경 | 실험 1 일부 |
| `rag/agents/retrieval_agent.py` | ADJACENT_DOMAINS 확장 | 실험 1 일부 |
| `rag/utils/config/domain_data.py` | finance_tax 키워드 추가, 인접 도메인 보강 | 검색 노이즈 |
| `rag/utils/legal_supplement.py` | _DOC_CHECK_LIMIT 8→12, _DOC_CONTENT_LIMIT 800→1200 | 검색 노이즈 |

---

## 7. 결과 파일

| 파일 | 내용 |
|------|------|
| `rag/evaluation/results/ragas_v4_optimized.json` | Baseline (3차) 결과 |
| `rag/evaluation/results/ragas_v4_phase4.json` | 실험 1 (A+B+C+D) 결과 |
| `rag/evaluation/results/ragas_v4_bcd_only.json` | 실험 2 (B/C/D only) 결과 |
| `rag/evaluation/results/ragas_v4_timeout_bm25fix.json` | 실험 3 (timeout + BM25 fix) 결과 |

---

## 부록: 1차~3차 평가 비교 (이전 보고서 내용)

### 3단계 평가 비교 (1차 → 2차 → 3차)

| 메트릭 | 1차 (gpt-4.1-mini) | 2차 (improved_v2) | 3차 (optimized) |
|--------|:-:|:-:|:-:|
| **Faithfulness** | 0.4723 | 0.5131 | **0.5648** |
| **Answer Relevancy** | **0.6752** | 0.6494 | 0.6267 |
| **Context Precision** | **0.6567** | 0.6584 | 0.5801 |
| **Context Recall** | 0.6023 | 0.5712 | **0.6215** |
| **Overall F1** | **0.4713** | 0.4419 | 0.4384 |
| **F1=0** | 15건 | - | 21건 |

### 핵심 개선 흐름

```
1차: F가 낮고 CR이 낮음
  ↓ 프롬프트 Grounding 강화 + Legal supplement
2차: F 개선, CR 소폭 하락
  ↓ 추가 프롬프트 + 검색 최적화
3차(Baseline): F 크게 개선, AR/CP는 하락 (트레이드오프)
  ↓ timeout 120s + law_common BM25 fix
실험 3: CP/CR 크게 개선 (평가 모델 다름)
```
