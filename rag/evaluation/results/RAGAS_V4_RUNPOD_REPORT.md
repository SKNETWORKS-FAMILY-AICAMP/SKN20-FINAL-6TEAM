# RAGAS V4 평가 리포트 — RunPod GPU 모드

> 평가일: 2026-02-20
> 데이터셋: `qa_test/ragas_dataset_v4.jsonl` (80문항, 12 페르소나)
> 평가 환경: **RunPod GPU** (bge-m3 임베딩, bge-reranker cross-encoder)
> RAGAS 평가 모델: `gpt-3.5-turbo` (max_tokens=4096)
> 결과 파일: `rag/evaluation/results/ragas_v4_runpod.json`

---

## 1. 핵심 요약

### 타임아웃 완전 해소

| 항목 | CPU 1차 | CPU 2차 | **RunPod GPU** |
|------|---------|---------|----------------|
| 유효 응답 | 68/80 | 60/80 | **80/80** |
| 타임아웃 | 12건 (15%) | 20건 (25%) | **0건 (0%)** |
| 평균 응답 시간 | - | - | **36.4초** |

> RunPod GPU 임베딩/리랭킹으로 전환 후 **타임아웃이 완전히 사라짐.**
> 이전 CPU 모드에서 타임아웃된 20건도 정상 처리되어, 80개 전체 질문에 대해 유효한 답변을 얻음.

### RAGAS 메트릭 (80문항 전체)

| 메트릭 | CPU 1차 (80문항) | CPU 2차 (80문항) | **RunPod (80문항)** | 변화 (2차 대비) |
|--------|-----------------|-----------------|---------------------|----------------|
| Faithfulness | 0.6437 | 0.6069 | **0.6770** | +0.0701 |
| Answer Relevancy | 0.8169 | 0.7162 | 0.5230 | -0.1932 |
| Context Precision | 0.9546 | 0.9466 | **0.9160** | -0.0306 |
| Context Recall | 0.6444 | 0.6447 | **0.7431** | **+0.0984** |

### 해석 시 주의사항

**RAGAS 평가 모델이 다릅니다.**

| 항목 | CPU 1차/2차 | RunPod |
|------|-----------|--------|
| RAGAS 평가 LLM | `gpt-4o-mini` | `gpt-3.5-turbo` |
| max_tokens | 8192 | 4096 |

- CPU와 RunPod의 메트릭을 **직접 비교하면 안 됩니다.** 평가 모델이 다르면 동일한 답변에 대해서도 점수가 달라집니다.
- 특히 **Answer Relevancy 하락 (0.72 → 0.52)**은 RAG 품질 저하가 아니라, gpt-3.5-turbo의 한국어 평가 능력 한계로 판단됩니다.
- **Context Recall 상승 (+0.10)**은 타임아웃 0건으로 80개 전체가 평가에 포함된 효과가 큽니다.

---

## 2. RunPod GPU 도입 효과

### 2.1 응답 시간 대폭 개선

| 구간 | 건수 | 비율 |
|------|------|------|
| 30초 이내 | 40건 | 50.0% |
| 30~60초 | 35건 | 43.8% |
| 60~120초 | 2건 | 2.5% |
| 120초 초과 | 3건 | 3.8% |
| **타임아웃 (300초)** | **0건** | **0%** |

- 평균: **36.4초** / 중간값: **30.4초**
- 최소: 14.3초 / 최대: 166.6초
- 93.8%의 질문이 60초 이내 처리

### 2.2 검색 품질 (Context Recall)

| 항목 | CPU 2차 | RunPod | 설명 |
|------|---------|--------|------|
| Context Recall | 0.6447 | **0.7431** | +0.10 향상 |
| 0.8 이상 | - | 50건 (62.5%) | ground truth 대부분 검색 |
| 0.5 미만 | - | 17건 (21.3%) | 개선 여지 있음 |

> RunPod GPU 리랭킹이 적용되면서, 검색된 문서의 관련성이 개선됨.
> 타임아웃 없이 모든 질문이 검색 과정을 완주한 것도 Recall 상승에 기여.

### 2.3 Faithfulness (사실 일관성)

| 항목 | 값 |
|------|-----|
| 평균 | 0.6770 |
| 0.8 이상 | 44건 (55.0%) |
| 0.5 미만 | 24건 (30.0%) |
| 0.0 (완전 실패) | 6건 (7.5%) |

> Faithfulness 0인 6건은 복합 도메인 질문 (세무+법률, 노무+법률 등)으로,
> 여러 도메인을 넘나드는 답변에서 컨텍스트 밖 정보를 인용한 사례.

---

## 3. 인프라 구성

### 실행 환경

```
[ChromaDB]  Docker (localhost:8200)  ← 벡터 검색
     ↓
[RAG 파이프라인]  로컬 Python
     ├── 임베딩: RunPod GPU (bge-m3)
     ├── 리랭킹: RunPod GPU (bge-reranker)
     ├── LLM 답변 생성: OpenAI GPT-4o
     └── RAGAS 평가: OpenAI gpt-3.5-turbo
```

### 벡터 DB 컬렉션

| 컬렉션 | 도메인 |
|--------|--------|
| startup_funding_db | 창업/지원사업 |
| finance_tax_db | 재무/세무 |
| hr_labor_db | 인사/노무 |
| law_common_db | 법률 (법령, 판례, 유권해석) |

### 배치 모드 최적화 설정

평가 속도를 위해 아래 기능을 비활성화하고 실행:

| 기능 | 상태 | 이유 |
|------|------|------|
| 내부 LLM 평가 | OFF | RAGAS 외부 평가로 대체 |
| 법률 보충 검색 | OFF | 순수 도메인 검색 성능 측정 |
| 평가 후 재시도 | OFF | 1회 검색 품질 측정 |
| 단계적 재시도 | OFF | 동일 |

---

## 4. 비용

| 항목 | 비용 |
|------|------|
| RunPod GPU (임베딩/리랭킹) | ~$0.5 (80건 처리) |
| OpenAI GPT-4o (RAG 답변 생성) | ~$3~5 (80건) |
| OpenAI gpt-3.5-turbo (RAGAS 평가) | ~$1~2 (320건 평가) |
| **합계** | **~$5~8** |

---

## 5. 결론 및 다음 단계

### 성과

1. **타임아웃 완전 해소**: CPU 모드의 최대 병목이었던 타임아웃(25%)이 RunPod GPU로 0%로 해결됨
2. **Context Recall 목표 달성**: 0.6447 → 0.7431로 개선, 목표(0.70) 초과 달성
3. **안정적인 응답 시간**: 평균 36초, 94%가 60초 이내

### 한계

1. **RAGAS 평가 모델 차이**: gpt-3.5-turbo로 평가하여 CPU 결과(gpt-4o-mini)와 직접 비교 불가
2. **Answer Relevancy 저평가**: gpt-3.5-turbo의 한국어 이해력 한계로 실제보다 낮게 측정된 것으로 추정
3. **Faithfulness 0 케이스**: 복합 도메인 질문 6건에서 컨텍스트 외 정보 인용 문제 잔존

### 다음 단계

| # | 작업 | 우선순위 |
|---|------|---------|
| 1 | gpt-4o-mini(max_tokens=4096)로 RAGAS 재평가하여 CPU와 동일 조건 비교 | 높음 |
| 2 | 법률 보충 검색 ON 상태에서 재평가 (실제 서비스 환경) | 중간 |
| 3 | Faithfulness 0인 6건 원인 분석 및 프롬프트 개선 | 중간 |
| 4 | law_common BM25 인덱스 "too many SQL variables" 경고 해결 | 낮음 |

---

## 부록: 파일 목록

| 파일 | 설명 |
|------|------|
| `rag/evaluation/results/ragas_v4_runpod.json` | RunPod GPU 평가 결과 (본 리포트) |
| `rag/evaluation/results/ragas_v4_results.json` | CPU 1차 평가 결과 |
| `rag/evaluation/results/ragas_v4_improved.json` | CPU 2차 평가 결과 (프롬프트 개선 후) |
| `rag/evaluation/results/RAGAS_V4_REPORT.md` | CPU 평가 리포트 |
| `qa_test/ragas_dataset_v4.jsonl` | 테스트 데이터셋 (80문항) |
