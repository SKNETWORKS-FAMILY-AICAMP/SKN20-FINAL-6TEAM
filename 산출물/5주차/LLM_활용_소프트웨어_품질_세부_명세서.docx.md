  
**SK네트웍스 Family AI 과정 20기**  
**LLM 활용 소프트웨어 품질 세부 명세서**

| 산출물 단계 | 모델링 및 평가 |
| :---- | :---- |
| **평가 산출물** | LLM 활용 소프트웨어 품질 세부 명세서 |
| **제출 일자** | 2026-02-09 |
| **깃허브 경로** | https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN20-FINAL-6TEAM/tree/main |
| **작성 팀원** | 정소영 |

# **1\. LLM 활용 개요**

## **1.1 프로젝트 배경**

본 프로젝트는 창업 초기 기업의 재무, 노무, 지원사업 관련 질의에 대한 AI 상담 서비스를 제공하기 위해 개발되었습니다. 기존 챗봇의 한계인 환각(Hallucination) 문제를 해결하고 신뢰도 높은 정보를 제공하기 위하여 RAG(Retrieval-Augmented Generation) 아키텍처를 채택하였으며, 최신 LLM 기술을 활용하여 실시간 상담 품질을 보장하고자 합니다.

## **1.2 활용 LLM 모델**

| 모델명 | 제공사 | 용도 | 선정 이유 |
| :---- | :---- | :---- | :---- |
| GPT-4o-mini | OpenAI | 답변 생성, 질문 분해, 품질 평가 | 빠른 응답 속도, 높은 정확도, 비용 효율성 |
| BAAI/bge-m3 | BAAI (오픈소스) | 벡터 임베딩 | 한국어 성능 우수, 다국어 지원, 무료 사용 |

## **1.3 기술 스택**

| 카테고리 | 기술 스택 |
| :---- | :---- |
| 언어/런타임 | Python 3.10+ |
| 웹 프레임워크 | FastAPI (비동기 처리) |
| LLM 프레임워크 | LangChain, LangGraph (Agentic RAG) |
| 벡터DB | ChromaDB (로컬 벡터 저장소) |
| 평가 프레임워크 | RAGAS (RAG 평가 라이브러리) |

# **2\. 프롬프트 엔지니어링을 통한 품질 관리**

본 시스템은 LLM 모델을 직접 학습하지 않고, 프롬프트 설계를 통해 출력 품질을 제어하고 있습니다. 모든 에이전트의 시스템 프롬프트는 utils/prompts.py에서 중앙 관리되고, 환각 방지와 출처 명시를 핵심 원칙으로 합니다.

## **2.1 환각(Hallucination) 방지 전략**

모든 도메인 에이전트 프롬프트에 동일한 환각 방지 규칙을 적용하고 있습니다:

| 규칙 | 프롬프트 구현 |
| :---- | :---- |
| **근거 없는 답변 금지** | "참고 자료에 없는 내용을 만들어내지 마세요" |
| **추론/추측 금지** | "확실하지 않은 정보를 추측하여 답변하지 마세요" (세율, 금액, 기한 등 구체적 명시) |
| **허위 정보 금지** | "법령, 절차, 금액 등을 임의로 지어내지 마세요" \+ "참고 자료가 부족하면 '해당 정보를 찾을 수 없습니다'라고 솔직히 답변하세요" |

## **2.2 출처 명시 강제 전략**

모든 답변에 출처를 강제하기 위해 응답 형식을 프롬프트에 명시하고 있습니다.

**프롬프트 구현 예시:**

| *\--- \[답변 근거\] \- \[1\] 참고 자료 제목 \- \[2\] 참고 자료 제목 \---  주의: 본문 내용이 아닌 참고 자료의 제목만 작성하세요. 참고 자료를 전혀 사용하지 않은 경우에만 "답변 근거 없음"이라고 작성하세요.* |
| :---- |

**효과:** LLM이 답변 마지막에 출처를 명시하도록 유도하여, 검색된 문서를 실제로 활용했는지 검증

## **2.3 참고 자료 활용 원칙 명시**

검색된 문서를 반드시 활용하도록 프롬프트에 강제 규칙을 삽입합니다:

| 참고 자료 활용 원칙: \- 위의 참고 자료에 관련 내용이 있다면 반드시 해당 내용을 기반으로 답변하세요 \- 참고 자료의 정보를 직접 인용하고, \[번호\] 형식으로 출처를 표기하세요 \- 참고 자료에 관련 내용이 있는데도 "답변 근거 없음"이라고 작성하면 안 됩니다 \- 참고 자료를 먼저 확인하고, 해당 내용을 바탕으로 답변을 구성하세요 |
| :---- |

# **3\. 복합 질문 분해 로직 개선**

utils/question\_decomposer.py 모듈은 여러 도메인에 걸친 복합 질문을 단일 도메인 질문들로 분해하며, 다음과 같은 핵심 개선 사항을 포함합니다:

## **3.1 대화 이력 활용 및 맥락 보존**

**문제:** "그거 세금은 어떻게 되나요?" 같은 대명사나 생략된 주어가 있는 질문 처리 불가

**해결:** 최근 3턴의 대화 이력을 프롬프트에 포함하여 생략된 대명사/주어를 파악하도록 개선

| 구현 | 효과 |
| :---- | :---- |
| MAX\_HISTORY\_TURNS \= 3 최근 3턴(user+assistant 쌍)만 프롬프트에 포함 | 대명사/생략 주어를 구체화하여 독립적으로 이해 가능한 질문 생성 |

 

**코드 예시:**

| def \_format\_history(history, max\_turns=3): 	recent \= history\[-(max\_turns \* 2):\] 	return "\\n".join(\[f"사용자: {msg\['content'\]}" for msg in recent\]) |
| :---- |

## **3.2 캐시 시스템 도입**

**문제:** 동일한 복합 질문에 대해 매번 LLM 호출 시 비용 및 지연 발생

**해결:** LRU 캐시 적용으로 분해 결과를 재사용하여 호출 비용 및 지연을 개선

| 설정 | 값 | 설명 |
| :---- | :---- | :---- |
| max\_size | 200개 | 최대 200개 질문 분해 결과 저장 |
| default\_ttl | 3600초 (1시간) | 1시간 경과 시 자동 삭제 |

**캐시 키 생성 로직:**

| def \_build\_cache\_key(query, domains, history): 	key\_parts \= \[query, ",".join(sorted(domains))\]     \# 최근 1턴의 assistant 응답을 키에 포함 (맥락 변화 감지)     return hashlib.md5(raw\_key.encode()).hexdigest() |
| :---- |

## **3.3 에러 핸들링 및 Fallback**

**문제:** LLM 응답이 JSON 형식이 아니거나 파싱 실패 시 서비스 중단

**해결:** 예외 처리 후 각 도메인에 원본 질문 전달 (Graceful Degradation)

try:     response \= chain.invoke(variables)     sub\_queries \= self.\_parse\_response(response, detected\_domains) except (json.JSONDecodeError, Exception) as e:     \# 실패 시 각 도메인에 원본 질문 전달 	return \[SubQuery(domain=domain, query=query) for domain in detected\_domains\]

**효과:** LLM 파싱 실패해도 검색은 계속 진행되어 서비스 안정성 확보

# **4\. RAG 아키텍처 및 LLM 통합**

## **4.1 LangGraph 기반 5단계 파이프라인**

본 시스템은 LangGraph StateGraph를 활용하여 질문 처리부터 답변 생성까지 5단계 파이프라인으로 구성되어 있습니다. 각 단계는 독립적으로 동작하며, 상태 관리를 통해 이전 단계의 결과를 다음 단계로 전달합니다.

| 단계 | 모듈 | 기능 및 LLM 활용 |
| :---- | :---- | :---- |
| 1\. 분류 | VectorDomainClassifier | 키워드 매칭 \+ 벡터 유사도 기반 도메인 분류 (창업/재무/노무). 도메인 외 질문 시 거부 응답 반환 |
| 2\. 분해 | QuestionDecomposer (GPT-4o-mini) | 복합 질문을 도메인별 하위 질문으로 분해 (예: "창업+세무" → 2개 SubQuery) |
| 3\. 검색 | RAGChain \+ Evaluator | 벡터DB 병렬 검색 → 규칙 기반 평가 → 실패 시 Multi-Query 재검색 |
| 4\. 생성 | GPT-4o-mini | 검색된 문서 기반 답변 생성 \+ 복수 도메인 응답 통합 \+ 액션 제안 |
| 5\. 평가 | RAGAS \+ GPT-4o-mini | 답변 품질 평가 및 로깅 (정확성, 완성도, 관련성, 출처 명시) |

## **4.2 도메인별 에이전트 구조**

본 시스템은 3개의 전문 도메인 에이전트와 2개의 보조 에이전트로 구성되어 있습니다:

| 에이전트 | 담당 도메인 | VectorDB |
| :---- | :---- | :---- |
| Router | 질문 분류, 에이전트 조율 | \- |
| Startup & Funding | 창업/지원사업/마케팅 | startup\_funding\_db \+ law\_common\_db |
| Finance & Tax | 세무/회계/재무 | finance\_tax\_db \+ law\_common\_db |
| HR & Labor | 노무/인사/법률 | hr\_labor\_db \+ law\_common\_db |
| Evaluator | 답변 품질 평가 | \- |
| Action Executor | 문서 생성 (PDF/HWP) | \- |

**공통 VectorDB:** law\_common\_db (법령/해석례, 모든 에이전트 공유)

# **5\. 3중 품질 관리 체계**

## **5.1 검색 품질 평가 (규칙 기반)**

**모듈:** RuleBasedRetrievalEvaluator

**목적:** 벡터DB 검색 결과의 충분성을 판단하여 재검색 필요 여부 결정

| 평가 기준 | 임계값 | 조치 |
| :---- | :---- | :---- |
| 문서 수 | MIN\_RETRIEVAL\_DOC\_COUNT ≥ 2 | 미달 시 Multi-Query 재검색 |
| 키워드 매칭 | 질문 키워드가 문서에 포함되었는지 확인 | 매칭 실패 시 Multi-Query 재검색 |
| 유사도 점수 | MIN\_SIMILARITY\_SCORE ≥ 0.3 | 미달 시 Multi-Query 재검색 |

## **5.2 LLM 기반 답변 품질 평가**

**모듈:** Evaluator (GPT-4o-mini)

**목적:** 생성된 답변의 품질을 5개 기준으로 평가 (선택적 활성화)

| 평가 기준 | 설명 | 배점 |
| :---- | :---- | :---- |
| 정확성 | 제공된 정보가 사실에 부합하는지 | 20점 |
| 완성도 | 질문에 대해 충분히 답변했는지 | 20점 |
| 관련성 | 질문 의도에 맞는 답변인지 | 20점 |
| 출처 명시 | 법령/규정 인용 시 출처가 있는지 | 20점 |
| 가독성 | 이해하기 쉽게 구조화되었는지 | 20점 |

**합격 기준:** 70점 이상 (100점 만점)

**활성화 설정:** ENABLE\_LLM\_EVALUATION=true (기본: false)

## **5.3 RAGAS 정량 평가**

**모듈:** RagasEvaluator

**목적:** RAG 시스템의 정량적 품질을 메트릭으로 측정하여 로깅 (재시도 없음)

| 메트릭 | 설명 |
| :---- | :---- |
| Faithfulness | 답변이 검색된 문서 내용에 충실한지 (환각 방지) |
| Answer Relevancy | 답변이 질문과 얼마나 관련성이 높은지 |
| Context Precision | 검색된 문서 중 관련 있는 문서의 비율 |
| Context Recall | 질문에 필요한 정보를 얼마나 많이 검색했는지 |

 **로그 위치:** logs/ragas.log (JSON Lines 형식)

**활성화 설정:** ENABLE\_RAGAS\_EVALUATION=true (기본: true)

# **6\. Multi-Query 재검색 전략**

## **6.1 재검색 트리거 조건**

규칙 기반 검색 평가에서 실패 판정 시 자동으로 Multi-Query 재검색을 실행합니다:

| 실패 조건 | 문제점 | 재검색 전략 |
| :---- | :---- | :---- |
| 문서 수 \< 2 | 검색 결과 부족 | 3개 변형 쿼리 생성 |
| 키워드 미매칭 | 질문과 문서 불일치 | 다양한 표현으로 재검색 |
| 유사도 \< 0.3 | 관련성 낮은 문서 | 의미 확장 쿼리 생성 |

## **6.2 Multi-Query 생성 방식**

GPT-4o-mini를 활용하여 원본 질문의 3가지 변형 쿼리를 생성합니다:

**예시 질문:** "창업 절차가 궁금합니다"

• 변형 1: 사업자 등록은 어떻게 하나요?

• 변형 2: 개인 사업자로 창업하려면 무엇이 필요한가요?

• 변형 3: 법인 설립 절차를 알려주세요

# **7\. 응답 품질 측정 결과**

## **7.1 벡터 임베딩 모델 벤치마크**

10개 임베딩 모델 중 BAAI/bge-m3를 최종 선정하였습니다. BAAI/bge-m3는 OpenAI 최신 모델 대비 우수한 한국어 성능을 보여주었습니다:

| 지표 | OpenAI 3-large | BGE-M3 | 성능 차이 |
| :---- | :---- | :---- | :---- |
| HR@5 | 33.33% | **42.00%** | **\+8.67%p** |
| Precision@5 | 6.93% | **8.53%** | **\+1.60%p** |
| Recall@5 | 31.78% | **39.22%** | **\+7.44%p** |

## **7.2 도메인별 검색 성능**

| 도메인 | HR@5 | Precision@5 | MRR |
| :---- | :---- | :---- | :---- |
| 창업/지원 | **64.00%** | 13.20% | 0.5438 |
| 재무/세무 | 24.00% | 4.80% | 0.1329 |
| 인사/노무 | 38.00% | 7.60% | 0.2824 |
| **평균** | **42.00%** | **8.53%** | **0.3197** |

# **8\. 결론 및 향후 개선 방안**

## **8.1 LLM 활용 성과**

• 환각(Hallucination) 최소화: 프롬프트 엔지니어링 (근거 없는 답변 금지, 출처 명시 강제)으로 신뢰도 향상

• 한국어 특화 성능: 오픈소스 임베딩 모델로 OpenAI 대비 \+8.67%p 높은 검색 성능

• 3중 품질 관리: 검색 평가 → LLM 평가 → RAGAS 로깅으로 품질 개선 및 보장

• 복합 질문 분해 개선: 대화 이력 활용 \+ 캐시 시스템 \+ 에러 핸들링으로 안정성 확보

• Multi-Query 재검색: 규칙 기반 실패 감지 및 자동 재시도로 검색 성공률 향상

## **8.2 향후 개선 방안**

| 개선 영역 | 개선 방안 |
| :---- | :---- |
| 도메인 이해도 | 전문 용어 사전을 구축, ToolTip 기능으로 제공 |
| 응답 속도 | 벡터DB 인덱스 최적화, 캐싱 전략 강화 |
| 사용자 피드백 | 답변 만족도 수집 및 프롬프트 개선에 반영 |

 

## **8.3 최종 정리**

본 프로젝트는 LLM 모델을 직접 학습하거나 파인튜닝하지 않고도 고품질 AI 상담 서비스를 구축할 수 있음을 보여줍니다. 또한 프롬프트 엔지니어링을 통한 환각 방지, 복합 질문 분해 로직 개선, RAG 아키텍처, 도메인별 벡터DB 분리, 3중 품질 평가 체계를 통해 신뢰도 높은 답변을 생성하며, 오픈소스 임베딩 모델로 상용 모델 대비 우수한 한국어 성능을 확보하였습니다.

향후 지속적인 고도화를 통해 창업 기업을 위한 최고의 AI 상담 서비스로 발전시켜 나갈 계획입니다.

