  
**SK네트웍스 Family AI 과정 20기**  
 **데이터 정제 및 가공 결과서**

| 산출물 단계 | 데이터 전처리 |
| :---: | :---- |
| **평가 산출물** | 데이터 정제 및 가공 결과서 |
| **제출 일자** | 2026-01-29 |
| **깃허브 경로** | https://github.com/SKNETWORKS-FAMILY-AICAMP/SKN20-FINAL-6TEAM/tree/main |
| **작성 팀원** | 정소영 |

# **1\. 전처리 방법 비교 및 선정 이유**

| 방법론 | 종류 | 선정 이유 |
| :---- | :---- | :---- |
| BeautifulSoup4 | HTML 파싱 라이브러리 | HTML 태그 제거에 강함, Python 생태계와 호환성 우수 |
| 정규표현식 | 패턴 기반 텍스트 처리 | 빠른 처리 속도, 특수문자 제거에 적합 |

• 실험 방법 수: 총 2종

• 최종 선정 방법: BeautifulSoup4 \+ 정규표현식 조합

# **2\. 전처리 프로세스 및 아키텍처**

**2.1 전처리 파이프라인 구조**

• 데이터 수집 → 텍스트 정제 → 메타데이터 부착 → 구조적 분할(청킹) → 벡터DB 저장

**2.2 구성 요소 설명:**

| 단계 | 역할 | 구성 요소 |
| :---- | :---- | :---- |
| 수집 | crawlers/에서 도메인별 수집(bizinfo, kstartup, law, tax) | 공공데이터 API/웹 크롤링  |
| 정제 | preprocessors/cleaner.py (HTML 태그 제거, 특수문자/공백 정규화, UTF-8 통일)  | BeautifulSoup4, requests/httpx  |
| 메타데이터 추가 | 검색 정확도 향상 데이터에 따라 메타 데이터의 형식을 구 | 수집 시점의 도메인 정보와 문서 출처, 법령 시행일 등을 메타데이터로 저장, 질의 시 필터링 및 도메인별 검색이 가능하도록 구성 |
| 분할 | preprocessors/chunker.py \+ LangChain splitter (chunk\_size/overlap) | text splitter를 활용하여 chunk size와 overlap을 설정, 의미 단위가 끊기지 않도록 조문/문단 기준으로 분할 |
| 임베딩 | preprocessors/embedder.py에서 임베딩 생성 |  |
| 벡터 적재 | pipelines/ 및 scripts/sync\_vectordb.py, scripts/run\_all.py로 벡터DB(ChromaDB) 적재/동기화  | ChromaDB \+ host/port 환경변 |

# **3\. 전처리 설정 및 파라미터**

| 항목 | 값 |
| :---- | :---- |
| 원본 데이터 수 | 약 20,000 개 |
| 전처리 완료 데이터 수 | 약 9,588 개 |
| 인코딩 방식 | UTF-8 통일 |
| 사용 라이브러리 | BeautifulSoup4, pandas, LangChain |
| 벡터DB | ChromaDB |
| 품질 검증 기준 | HTML 태그 제거율 \> 99%, 중복율 \< 5% |

# **4\. 전처리 결과 및 품질 평가**

**4.1. 전처리 결과 요약**

| 도메인 | 정제율 | 중복 제거율 |
| :---- | :---- | :---- |
| 지원사업 | 99.22% | 100.0% |
| 법령 | 99.98% | 100.0% |
| 세무/회계 | 99.19% | 100.0% |
| 인사/노무 | 99.2% | 100.0% |

**4.2 전처리 평가 코드**

| """ 데이터 품질 배치 측정 스크립트 (시각화 포함) 여러 파일을 한 번에 측정 """ import json from pathlib import Path from collections import Counter import matplotlib.pyplot as plt  def measure\_quality(file\_path, save\_chart\=False):     """JSONL 파일의 품질 지표 측정"""         print(f"\\n📂 파일 분석 중: {Path(file\_path).name}")     print("="\*60)         \# JSONL 파일 읽기     documents \= \[\]     with open(file\_path, 'r', encoding\='utf-8') as f:         for line\_num, line in enumerate(f, 1):             try:                 doc \= json.loads(line.strip())                 documents.append(doc)             except json.JSONDecodeError as e:                 print(f"⚠️  라인 {line\_num} JSON 파싱 오류: {e}")         total\_docs \= len(documents)         \# \============================================     \# 1\. 정제율 측정 (HTML 태그 제거율)     \# \============================================     clean\_count \= 0     html\_tag\_found \= \[\]         for doc in documents:         content \= doc.get('content', '')         if '\<' not in content and '\>' not in content:             clean\_count \+= 1         else:             html\_tag\_found.append({                 'id': doc.get('id'),                 'title': doc.get('title', '')\[:50\]             })         clean\_rate \= (clean\_count / total\_docs) \* 100         \# \============================================     \# 2\. 중복 제거율 측정     \# \============================================     id\_list \= \[doc.get('id') for doc in documents\]     unique\_ids \= len(set(id\_list))     duplicate\_count \= total\_docs \- unique\_ids         id\_counter \= Counter(id\_list)     duplicates \= {id\_val: count for id\_val, count in id\_counter.items() if count \> 1}         dedup\_rate \= (unique\_ids / total\_docs) \* 100    … … |
| :---- |

**4.2. 그래프**

**4.3 해석 및 분석**

• 정제율이 가장 높은 도메인: 법령

• 주요 이슈: 데이터 일부에서 HTML 태그 발견 (\<% … %\> 등)

# **5\. 데이터 품질 검증 및 대응**

• 적용 기법:

| 규칙 | 적용 내용 | 예시 |
| :---- | :---- | ----- |
| HTML 태그 제거 | \<p\>, \<div\>, \<br\> 등 모든 태그 제거 | \<p\>텍스트\</p\> → 텍스트 |
| 특수문자 정규화 | \&nbsp;, &\#8203; 등 HTML 엔티티 제거 | \&nbsp; → 공백 |
| 연속 공백 제거 | 2개 이상 공백 → 1개로 통합 | "A B" → "A B" |
| 연속 줄바꿈 제거 | 3개 이상 줄바꿈 → 2개로 통합 | \\n\\n\\n → \\n\\n |
| 인코딩 통일 | 모든 문자를 UTF-8로 변환 | CP949 → UTF-8 |

* 전처리 전후 데이터 구조 변화

| 전처리 전 | 전처리 후 |
| ----- | ----- |
| \- 중첩된 JSON 구조 (4단계 depth) \- 불필요한 필드 다수 포함 (totCnt, inqireCo 등) \- 날짜 형식 불일치 (20230808, 2023-08-08 …) | \- 평탄화된 구조 (1단계 depth) \- 필수 필드만 유지 (id, type, domain, title, content, metadata) \- 날짜 형식 통일 (ISO 8601: YYYY-MM-DD) \- 통합 스키마 적용 (모두 동일 구조로 관리적으로 용이함) |

